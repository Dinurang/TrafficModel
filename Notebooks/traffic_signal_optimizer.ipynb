{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traffic Signal Optimization using Deep Learning\n",
    "\n",
    "This notebook implements a complete deep learning pipeline for optimizing traffic signal timings at a 4-way junction.\n",
    "\n",
    "**Features:**\n",
    "- Synthetic data generation for traffic scenarios\n",
    "- Feature engineering with temporal and interaction features\n",
    "- Deep neural network with constraints\n",
    "- Real-time prediction capability\n",
    "- Visualization and evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, callbacks\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Traffic Signal Optimizer Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrafficSignalOptimizer:\n",
    "    \"\"\"\n",
    "    Complete Deep Learning Pipeline for Traffic Signal Optimization\n",
    "    Predicts optimal green light times for 4 roads at a junction\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, max_cycle_time=120, min_green_time=15, max_green_time=60):\n",
    "        \"\"\"\n",
    "        Initialize the traffic signal optimizer\n",
    "        \n",
    "        Args:\n",
    "            max_cycle_time: Maximum total cycle time in seconds\n",
    "            min_green_time: Minimum green time per phase in seconds\n",
    "            max_green_time: Maximum green time per phase in seconds\n",
    "        \"\"\"\n",
    "        self.max_cycle_time = max_cycle_time\n",
    "        self.min_green_time = min_green_time\n",
    "        self.max_green_time = max_green_time\n",
    "        \n",
    "        # Initialize components\n",
    "        self.scaler = StandardScaler()\n",
    "        self.road_encoder = LabelEncoder()\n",
    "        self.day_encoder = LabelEncoder()\n",
    "        self.model = None\n",
    "        \n",
    "        # Model configuration\n",
    "        self.batch_size = 32\n",
    "        self.epochs = 200\n",
    "        self.learning_rate = 0.001\n",
    "        self.patience = 20\n",
    "    \n",
    "    def generate_synthetic_data(self, n_samples=50000):\n",
    "        \"\"\"\n",
    "        Generate synthetic training data for traffic signal optimization\n",
    "        \n",
    "        Args:\n",
    "            n_samples: Number of samples to generate\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame with synthetic traffic data\n",
    "        \"\"\"\n",
    "        print(f\"Generating {n_samples} synthetic samples...\")\n",
    "        \n",
    "        data = []\n",
    "        roads = ['North', 'South', 'East', 'West']\n",
    "        days = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "        priorities = {'Main': 3, 'Secondary': 2, 'Local': 1}\n",
    "        \n",
    "        for i in range(n_samples):\n",
    "            # Generate random timestamp\n",
    "            base_date = datetime(2024, 1, 1)\n",
    "            random_days = np.random.randint(0, 365)\n",
    "            random_hours = np.random.randint(0, 24)\n",
    "            random_minutes = np.random.randint(0, 60)\n",
    "            timestamp = base_date + timedelta(days=random_days, hours=random_hours, minutes=random_minutes)\n",
    "            \n",
    "            hour = timestamp.hour\n",
    "            minute = timestamp.minute\n",
    "            day = days[timestamp.weekday()]\n",
    "            date_str = timestamp.strftime('%Y-%m-%d')\n",
    "            \n",
    "            # Determine if it's peak hour\n",
    "            is_peak_hour = 1 if (7 <= hour <= 9) or (17 <= hour <= 19) else 0\n",
    "            \n",
    "            # Generate traffic data for each road\n",
    "            road_data = {}\n",
    "            optimal_times = []\n",
    "            total_flow = 0\n",
    "            \n",
    "            for road in roads:\n",
    "                # Generate realistic traffic patterns\n",
    "                if is_peak_hour:\n",
    "                    base_flow = np.random.uniform(30, 80)  # Higher during peak\n",
    "                else:\n",
    "                    base_flow = np.random.uniform(10, 40)  # Lower during off-peak\n",
    "                \n",
    "                # Add time-of-day variation\n",
    "                time_factor = 1 + 0.5 * np.sin(2 * np.pi * hour / 24)\n",
    "                flow_rate = base_flow * time_factor + np.random.normal(0, 5)\n",
    "                flow_rate = max(flow_rate, 0)\n",
    "                \n",
    "                # Determine if vehicles are waiting (higher probability during peak)\n",
    "                waiting_prob = 0.7 if is_peak_hour else 0.3\n",
    "                is_waiting = 1 if np.random.random() < waiting_prob else 0\n",
    "                \n",
    "                # Assign priority (Main roads get more traffic)\n",
    "                if road in ['North', 'South']:\n",
    "                    priority = 'Main'\n",
    "                else:\n",
    "                    priority = np.random.choice(['Main', 'Secondary', 'Local'], p=[0.3, 0.5, 0.2])\n",
    "                \n",
    "                road_data[f'{road}_is_waiting'] = is_waiting\n",
    "                road_data[f'{road}_flow_rate'] = flow_rate\n",
    "                road_data[f'{road}_priority'] = priorities[priority]\n",
    "                road_data[f'{road}_name'] = road\n",
    "                total_flow += flow_rate\n",
    "            \n",
    "            # Calculate optimal green times based on traffic conditions\n",
    "            for road in roads:\n",
    "                flow_ratio = road_data[f'{road}_flow_rate'] / max(total_flow, 1)\n",
    "                priority_factor = road_data[f'{road}_priority'] / 3\n",
    "                waiting_factor = road_data[f'{road}_is_waiting'] * 0.2\n",
    "                \n",
    "                # Base allocation\n",
    "                base_time = self.min_green_time\n",
    "                \n",
    "                # Dynamic adjustment\n",
    "                dynamic_time = (flow_ratio * 0.5 + priority_factor * 0.3 + waiting_factor) * (self.max_cycle_time - 4 * self.min_green_time)\n",
    "                \n",
    "                optimal_time = base_time + dynamic_time\n",
    "                \n",
    "                # Add some randomness\n",
    "                optimal_time += np.random.normal(0, 2)\n",
    "                optimal_time = np.clip(optimal_time, self.min_green_time, self.max_green_time)\n",
    "                optimal_times.append(optimal_time)\n",
    "            \n",
    "            # Normalize to fit cycle time\n",
    "            optimal_times = np.array(optimal_times)\n",
    "            if optimal_times.sum() > self.max_cycle_time:\n",
    "                optimal_times = optimal_times * (self.max_cycle_time / optimal_times.sum())\n",
    "            \n",
    "            # Round to practical values\n",
    "            optimal_times = np.round(optimal_times / 5) * 5\n",
    "            \n",
    "            # Create sample\n",
    "            sample = {\n",
    "                'timestamp': timestamp,\n",
    "                'date': date_str,\n",
    "                'day': day,\n",
    "                'time': f\"{hour:02d}:{minute:02d}\",\n",
    "                'hour': hour,\n",
    "                'minute': minute,\n",
    "                'is_peak_hour': is_peak_hour,\n",
    "                **road_data,\n",
    "                'optimal_time_North': optimal_times[0],\n",
    "                'optimal_time_South': optimal_times[1],\n",
    "                'optimal_time_East': optimal_times[2],\n",
    "                'optimal_time_West': optimal_times[3]\n",
    "            }\n",
    "            data.append(sample)\n",
    "            \n",
    "            if (i + 1) % 10000 == 0:\n",
    "                print(f\"Generated {i + 1} samples...\")\n",
    "        \n",
    "        df = pd.DataFrame(data)\n",
    "        print(f\"Data generation complete. Shape: {df.shape}\")\n",
    "        return df\n",
    "    \n",
    "    def prepare_features(self, df):\n",
    "        \"\"\"\n",
    "        Prepare features from raw data\n",
    "        \n",
    "        Args:\n",
    "            df: DataFrame with raw traffic data\n",
    "            \n",
    "        Returns:\n",
    "            X: Feature matrix\n",
    "            y: Target matrix\n",
    "            feature_names: List of feature names\n",
    "        \"\"\"\n",
    "        print(\"Preparing features...\")\n",
    "        \n",
    "        # Encode categorical features\n",
    "        df['road_name_encoded'] = self.road_encoder.fit_transform(df['North_name'])\n",
    "        df['day_encoded'] = self.day_encoder.fit_transform(df['day'])\n",
    "        \n",
    "        # Create cyclical time features\n",
    "        df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)\n",
    "        df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)\n",
    "        df['minute_sin'] = np.sin(2 * np.pi * df['minute'] / 60)\n",
    "        df['minute_cos'] = np.cos(2 * np.pi * df['minute'] / 60)\n",
    "        df['day_sin'] = np.sin(2 * np.pi * df['day_encoded'] / 7)\n",
    "        df['day_cos'] = np.cos(2 * np.pi * df['day_encoded'] / 7)\n",
    "        \n",
    "        # Feature engineering\n",
    "        roads = ['North', 'South', 'East', 'West']\n",
    "        \n",
    "        # 1. Basic features for each road\n",
    "        basic_features = []\n",
    "        for road in roads:\n",
    "            basic_features.extend([\n",
    "                f'{road}_is_waiting',\n",
    "                f'{road}_flow_rate',\n",
    "                f'{road}_priority'\n",
    "            ])\n",
    "        \n",
    "        # 2. Derived features\n",
    "        derived_features = [\n",
    "            'hour_sin', 'hour_cos',\n",
    "            'minute_sin', 'minute_cos',\n",
    "            'day_sin', 'day_cos',\n",
    "            'is_peak_hour'\n",
    "        ]\n",
    "        \n",
    "        # 3. Interaction features\n",
    "        df['total_flow'] = sum(df[f'{road}_flow_rate'] for road in roads)\n",
    "        df['avg_flow'] = df['total_flow'] / 4\n",
    "        df['max_flow'] = max(df[f'{road}_flow_rate'] for road in roads)\n",
    "        df['flow_imbalance'] = df['max_flow'] / df['avg_flow']\n",
    "        \n",
    "        # 4. Time-based features\n",
    "        df['morning_rush'] = ((df['hour'] >= 7) & (df['hour'] <= 9)).astype(int)\n",
    "        df['evening_rush'] = ((df['hour'] >= 17) & (df['hour'] <= 19)).astype(int)\n",
    "        df['night'] = ((df['hour'] >= 22) | (df['hour'] <= 5)).astype(int)\n",
    "        \n",
    "        interaction_features = [\n",
    "            'total_flow', 'avg_flow', 'max_flow', 'flow_imbalance',\n",
    "            'morning_rush', 'evening_rush', 'night'\n",
    "        ]\n",
    "        \n",
    "        # Combine all features\n",
    "        feature_columns = basic_features + derived_features + interaction_features\n",
    "        X = df[feature_columns].values\n",
    "        \n",
    "        # Target variables\n",
    "        target_columns = [f'optimal_time_{road}' for road in roads]\n",
    "        y = df[target_columns].values\n",
    "        \n",
    "        print(f\"Feature matrix shape: {X.shape}\")\n",
    "        print(f\"Target matrix shape: {y.shape}\")\n",
    "        \n",
    "        return X, y, feature_columns\n",
    "    \n",
    "    def build_model(self, input_dim):\n",
    "        \"\"\"\n",
    "        Build the deep learning model\n",
    "        \n",
    "        Args:\n",
    "            input_dim: Number of input features\n",
    "            \n",
    "        Returns:\n",
    "            Compiled Keras model\n",
    "        \"\"\"\n",
    "        print(\"Building deep learning model...\")\n",
    "        \n",
    "        model = keras.Sequential([\n",
    "            # Input layer\n",
    "            layers.Input(shape=(input_dim,)),\n",
    "            \n",
    "            # Hidden layers with batch normalization and dropout\n",
    "            layers.Dense(256, activation='relu'),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.Dropout(0.3),\n",
    "            \n",
    "            layers.Dense(128, activation='relu'),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.Dropout(0.3),\n",
    "            \n",
    "            layers.Dense(64, activation='relu'),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.Dropout(0.2),\n",
    "            \n",
    "            layers.Dense(32, activation='relu'),\n",
    "            \n",
    "            # Output layer (4 green times)\n",
    "            layers.Dense(4, activation='relu', name='output')\n",
    "        ])\n",
    "        \n",
    "        # Custom loss function with constraints\n",
    "        def constrained_loss(y_true, y_pred):\n",
    "            # Base MSE loss\n",
    "            mse_loss = keras.losses.mean_squared_error(y_true, y_pred)\n",
    "            \n",
    "            # Minimum green time constraint\n",
    "            min_violation = tf.reduce_mean(\n",
    "                tf.maximum(self.min_green_time - y_pred, 0)\n",
    "            )\n",
    "            \n",
    "            # Maximum green time constraint\n",
    "            max_violation = tf.reduce_mean(\n",
    "                tf.maximum(y_pred - self.max_green_time, 0)\n",
    "            )\n",
    "            \n",
    "            # Total cycle time constraint\n",
    "            total_time = tf.reduce_sum(y_pred, axis=1)\n",
    "            cycle_violation = tf.reduce_mean(\n",
    "                tf.maximum(total_time - self.max_cycle_time, 0)\n",
    "            )\n",
    "            \n",
    "            # Sum constraints violation\n",
    "            total_violation = min_violation + max_violation + cycle_violation\n",
    "            \n",
    "            # Weighted loss\n",
    "            return mse_loss + 0.1 * total_violation\n",
    "        \n",
    "        # Compile model\n",
    "        optimizer = keras.optimizers.Adam(learning_rate=self.learning_rate)\n",
    "        model.compile(\n",
    "            optimizer=optimizer,\n",
    "            loss=constrained_loss,\n",
    "            metrics=['mae', 'mse']\n",
    "        )\n",
    "        \n",
    "        model.summary()\n",
    "        return model\n",
    "    \n",
    "    def train(self, X, y, validation_split=0.2):\n",
    "        \"\"\"\n",
    "        Train the model\n",
    "        \n",
    "        Args:\n",
    "            X: Feature matrix\n",
    "            y: Target matrix\n",
    "            validation_split: Fraction of data for validation\n",
    "        \"\"\"\n",
    "        print(\"Training model...\")\n",
    "        \n",
    "        # Split data\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            X, y, test_size=validation_split, random_state=42\n",
    "        )\n",
    "        \n",
    "        # Scale features\n",
    "        X_train_scaled = self.scaler.fit_transform(X_train)\n",
    "        X_val_scaled = self.scaler.transform(X_val)\n",
    "        \n",
    "        # Build model\n",
    "        self.model = self.build_model(X_train.shape[1])\n",
    "        \n",
    "        # Callbacks\n",
    "        early_stopping = callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=self.patience,\n",
    "            restore_best_weights=True\n",
    "        )\n",
    "        \n",
    "        reduce_lr = callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=10,\n",
    "            min_lr=1e-6\n",
    "        )\n",
    "        \n",
    "        # Train model\n",
    "        history = self.model.fit(\n",
    "            X_train_scaled, y_train,\n",
    "            validation_data=(X_val_scaled, y_val),\n",
    "            epochs=self.epochs,\n",
    "            batch_size=self.batch_size,\n",
    "            callbacks=[early_stopping, reduce_lr],\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        self.training_history = history\n",
    "        print(\"Training complete!\")\n",
    "        return history\n",
    "    \n",
    "    def evaluate(self, X_test, y_test):\n",
    "        \"\"\"\n",
    "        Evaluate the model\n",
    "        \n",
    "        Args:\n",
    "            X_test: Test features\n",
    "            y_test: Test targets\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with evaluation metrics\n",
    "        \"\"\"\n",
    "        print(\"Evaluating model...\")\n",
    "        \n",
    "        # Scale test data\n",
    "        X_test_scaled = self.scaler.transform(X_test)\n",
    "        \n",
    "        # Make predictions\n",
    "        y_pred = self.model.predict(X_test_scaled)\n",
    "        \n",
    "        # Apply post-processing constraints\n",
    "        y_pred_constrained = np.array([self.apply_constraints(pred) for pred in y_pred])\n",
    "        \n",
    "        # Calculate metrics\n",
    "        metrics = {\n",
    "            'MAE': mean_absolute_error(y_test, y_pred_constrained),\n",
    "            'MSE': mean_squared_error(y_test, y_pred_constrained),\n",
    "            'RMSE': np.sqrt(mean_squared_error(y_test, y_pred_constrained))\n",
    "        }\n",
    "        \n",
    "        # Calculate constraint violations\n",
    "        total_times = np.sum(y_pred_constrained, axis=1)\n",
    "        min_violations = np.sum(y_pred_constrained < self.min_green_time)\n",
    "        max_violations = np.sum(y_pred_constrained > self.max_green_time)\n",
    "        cycle_violations = np.sum(total_times > self.max_cycle_time)\n",
    "        \n",
    "        metrics['min_violations_pct'] = (min_violations / (y_pred_constrained.size)) * 100\n",
    "        metrics['max_violations_pct'] = (max_violations / (y_pred_constrained.size)) * 100\n",
    "        metrics['cycle_violations_pct'] = (cycle_violations / len(y_pred_constrained)) * 100\n",
    "        \n",
    "        print(\"\\nEvaluation Metrics:\")\n",
    "        print(f\"MAE: {metrics['MAE']:.2f} seconds\")\n",
    "        print(f\"RMSE: {metrics['RMSE']:.2f} seconds\")\n",
    "        print(f\"Minimum time violations: {metrics['min_violations_pct']:.2f}%\")\n",
    "        print(f\"Maximum time violations: {metrics['max_violations_pct']:.2f}%\")\n",
    "        print(f\"Cycle time violations: {metrics['cycle_violations_pct']:.2f}%\")\n",
    "        \n",
    "        return metrics, y_pred_constrained\n",
    "    \n",
    "    def apply_constraints(self, times):\n",
    "        \"\"\"\n",
    "        Apply practical constraints to predicted times\n",
    "        \n",
    "        Args:\n",
    "            times: Predicted green times\n",
    "            \n",
    "        Returns:\n",
    "            Constrained times\n",
    "        \"\"\"\n",
    "        times = np.array(times)\n",
    "        \n",
    "        # Ensure minimum green time\n",
    "        times = np.maximum(times, self.min_green_time)\n",
    "        \n",
    "        # Ensure maximum green time\n",
    "        times = np.minimum(times, self.max_green_time)\n",
    "        \n",
    "        # Adjust total cycle time\n",
    "        total = np.sum(times)\n",
    "        if total > self.max_cycle_time:\n",
    "            times = times * (self.max_cycle_time / total)\n",
    "        \n",
    "        # Round to practical values (multiples of 5 seconds)\n",
    "        times = np.round(times / 5) * 5\n",
    "        \n",
    "        # Final bounds check\n",
    "        times = np.clip(times, self.min_green_time, self.max_green_time)\n",
    "        \n",
    "        return times\n",
    "    \n",
    "    def predict_single(self, traffic_data):\n",
    "        \"\"\"\n",
    "        Predict optimal times for a single traffic scenario\n",
    "        \n",
    "        Args:\n",
    "            traffic_data: Dictionary with current traffic data\n",
    "            \n",
    "        Returns:\n",
    "            Optimized green times for each road\n",
    "        \"\"\"\n",
    "        # Convert to DataFrame for feature preparation\n",
    "        df = pd.DataFrame([traffic_data])\n",
    "        \n",
    "        # Prepare features (single sample)\n",
    "        X, _, _ = self.prepare_features(df)\n",
    "        \n",
    "        # Scale\n",
    "        X_scaled = self.scaler.transform(X)\n",
    "        \n",
    "        # Predict\n",
    "        raw_prediction = self.model.predict(X_scaled, verbose=0)[0]\n",
    "        \n",
    "        # Apply constraints\n",
    "        optimized_times = self.apply_constraints(raw_prediction)\n",
    "        \n",
    "        return optimized_times\n",
    "    \n",
    "    def plot_training_history(self):\n",
    "        \"\"\"Plot training history\"\"\"\n",
    "        if not hasattr(self, 'training_history'):\n",
    "            print(\"No training history available\")\n",
    "            return\n",
    "        \n",
    "        history = self.training_history.history\n",
    "        \n",
    "        fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "        \n",
    "        # Plot loss\n",
    "        axes[0].plot(history['loss'], label='Training Loss')\n",
    "        axes[0].plot(history['val_loss'], label='Validation Loss')\n",
    "        axes[0].set_xlabel('Epoch')\n",
    "        axes[0].set_ylabel('Loss')\n",
    "        axes[0].set_title('Training and Validation Loss')\n",
    "        axes[0].legend()\n",
    "        axes[0].grid(True)\n",
    "        \n",
    "        # Plot MAE\n",
    "        axes[1].plot(history['mae'], label='Training MAE')\n",
    "        axes[1].plot(history['val_mae'], label='Validation MAE')\n",
    "        axes[1].set_xlabel('Epoch')\n",
    "        axes[1].set_ylabel('MAE (seconds)')\n",
    "        axes[1].set_title('Training and Validation MAE')\n",
    "        axes[1].legend()\n",
    "        axes[1].grid(True)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def plot_predictions_vs_actual(self, y_true, y_pred):\n",
    "        \"\"\"Plot predictions vs actual values\"\"\"\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "        roads = ['North', 'South', 'East', 'West']\n",
    "        \n",
    "        for idx, (ax, road) in enumerate(zip(axes.flat, roads)):\n",
    "            ax.scatter(y_true[:, idx], y_pred[:, idx], alpha=0.5)\n",
    "            ax.plot([self.min_green_time, self.max_green_time], \n",
    "                   [self.min_green_time, self.max_green_time], \n",
    "                   'r--', label='Perfect Prediction')\n",
    "            ax.set_xlabel(f'Actual {road} Time (s)')\n",
    "            ax.set_ylabel(f'Predicted {road} Time (s)')\n",
    "            ax.set_title(f'{road} Road: Predictions vs Actual')\n",
    "            ax.legend()\n",
    "            ax.grid(True)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def save_model(self, path='traffic_signal_model'):\n",
    "        \"\"\"Save the model and preprocessing objects\"\"\"\n",
    "        import joblib\n",
    "        \n",
    "        # Save model\n",
    "        self.model.save(f'{path}.h5')\n",
    "        \n",
    "        # Save preprocessing objects\n",
    "        joblib.dump({\n",
    "            'scaler': self.scaler,\n",
    "            'road_encoder': self.road_encoder,\n",
    "            'day_encoder': self.day_encoder\n",
    "        }, f'{path}_preprocessors.pkl')\n",
    "        \n",
    "        print(f\"Model saved to {path}.h5\")\n",
    "        print(f\"Preprocessors saved to {path}_preprocessors.pkl\")\n",
    "    \n",
    "    def load_model(self, path='traffic_signal_model'):\n",
    "        \"\"\"Load the model and preprocessing objects\"\"\"\n",
    "        import joblib\n",
    "        \n",
    "        # Load model\n",
    "        self.model = keras.models.load_model(f'{path}.h5', custom_objects={'constrained_loss': None})\n",
    "        \n",
    "        # Load preprocessing objects\n",
    "        preprocessors = joblib.load(f'{path}_preprocessors.pkl')\n",
    "        self.scaler = preprocessors['scaler']\n",
    "        self.road_encoder = preprocessors['road_encoder']\n",
    "        self.day_encoder = preprocessors['day_encoder']\n",
    "        \n",
    "        print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Initialize Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the optimizer\n",
    "optimizer = TrafficSignalOptimizer(\n",
    "    max_cycle_time=120,\n",
    "    min_green_time=15,\n",
    "    max_green_time=60\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Generate Synthetic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic data\n",
    "data = optimizer.generate_synthetic_data(n_samples=20000)\n",
    "\n",
    "# Display sample data\n",
    "print(\"\\nSample Data:\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Prepare Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features\n",
    "X, y, feature_names = optimizer.prepare_features(data)\n",
    "\n",
    "print(\"\\nFeature names:\")\n",
    "for i, name in enumerate(feature_names):\n",
    "    print(f\"{i+1}. {name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history = optimizer.train(X, y, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualize Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "optimizer.plot_training_history()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Evaluate model\n",
    "metrics, y_pred = optimizer.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Visualize Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot predictions vs actual\n",
    "optimizer.plot_predictions_vs_actual(y_test[:100], y_pred[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Real-time Prediction Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample traffic scenario\n",
    "sample_traffic = {\n",
    "    'timestamp': datetime(2024, 1, 15, 8, 30),  # Monday morning peak\n",
    "    'date': '2024-01-15',\n",
    "    'day': 'Monday',\n",
    "    'time': '08:30',\n",
    "    'hour': 8,\n",
    "    'minute': 30,\n",
    "    'is_peak_hour': 1,\n",
    "    'North_is_waiting': 1,\n",
    "    'North_flow_rate': 65.2,\n",
    "    'North_priority': 3,\n",
    "    'North_name': 'North',\n",
    "    'South_is_waiting': 1,\n",
    "    'South_flow_rate': 58.7,\n",
    "    'South_priority': 3,\n",
    "    'South_name': 'South',\n",
    "    'East_is_waiting': 0,\n",
    "    'East_flow_rate': 22.3,\n",
    "    'East_priority': 2,\n",
    "    'East_name': 'East',\n",
    "    'West_is_waiting': 1,\n",
    "    'West_flow_rate': 31.5,\n",
    "    'West_priority': 2,\n",
    "    'West_name': 'West'\n",
    "}\n",
    "\n",
    "# Get optimized green times\n",
    "optimized_times = optimizer.predict_single(sample_traffic)\n",
    "\n",
    "print(\"Sample Traffic Scenario:\")\n",
    "print(f\"Time: {sample_traffic['time']} on {sample_traffic['day']}\")\n",
    "print(\"\\nTraffic Conditions:\")\n",
    "print(f\"  North: Waiting={sample_traffic['North_is_waiting']}, \"\n",
    "      f\"Flow={sample_traffic['North_flow_rate']:.1f} vehicles/min, \"\n",
    "      f\"Priority={sample_traffic['North_priority']}\")\n",
    "print(f\"  South: Waiting={sample_traffic['South_is_waiting']}, \"\n",
    "      f\"Flow={sample_traffic['South_flow_rate']:.1f} vehicles/min, \"\n",
    "      f\"Priority={sample_traffic['South_priority']}\")\n",
    "print(f\"  East: Waiting={sample_traffic['East_is_waiting']}, \"\n",
    "      f\"Flow={sample_traffic['East_flow_rate']:.1f} vehicles/min, \"\n",
    "      f\"Priority={sample_traffic['East_priority']}\")\n",
    "print(f\"  West: Waiting={sample_traffic['West_is_waiting']}, \"\n",
    "      f\"Flow={sample_traffic['West_flow_rate']:.1f} vehicles/min, \"\n",
    "      f\"Priority={sample_traffic['West_priority']}\")\n",
    "\n",
    "print(\"\\nOptimized Green Times:\")\n",
    "roads = ['North', 'South', 'East', 'West']\n",
    "for road, time in zip(roads, optimized_times):\n",
    "    print(f\"  {road} road: {time:.0f} seconds\")\n",
    "    \n",
    "print(f\"\\nTotal cycle time: {np.sum(optimized_times):.0f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "optimizer.save_model('traffic_signal_optimizer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Load Model (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To load a previously saved model:\n",
    "# new_optimizer = TrafficSignalOptimizer()\n",
    "# new_optimizer.load_model('traffic_signal_optimizer')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
